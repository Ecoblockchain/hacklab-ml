{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we're going to learn about how we use complex probability distributions to generate data. In other words, we're going to generate plausible, fake text from a body of text. To help out, we're going to need Python's NLTK and Numpy libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, random, numpy, textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'austen-emma.txt',\n",
       " u'austen-persuasion.txt',\n",
       " u'austen-sense.txt',\n",
       " u'bible-kjv.txt',\n",
       " u'blake-poems.txt',\n",
       " u'bryant-stories.txt',\n",
       " u'burgess-busterbrown.txt',\n",
       " u'carroll-alice.txt',\n",
       " u'chesterton-ball.txt',\n",
       " u'chesterton-brown.txt',\n",
       " u'chesterton-thursday.txt',\n",
       " u'edgeworth-parents.txt',\n",
       " u'melville-moby_dick.txt',\n",
       " u'milton-paradise.txt',\n",
       " u'shakespeare-caesar.txt',\n",
       " u'shakespeare-hamlet.txt',\n",
       " u'shakespeare-macbeth.txt',\n",
       " u'whitman-leaves.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has a neat corpus module with sets of test data to use. Within the files for project gutenberg, we've got a nice selection of classics to choose from. We'll be choosing \"Moby Dick\" because it is particularly long. Our first task is to read the data into a Python string and split it into words so we can process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training = nltk.corpus.gutenberg.open('melville-moby_dick.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_words = training.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will take a closer look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'[Moby'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_words[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, well that's a problem. The first words of this file look like a header. So let's move a little ways down the list until we find a sentence start *after* the header. With a large enough body of text this won't matter, but for our purposes let's make it easier on our selves by making sure our data is higher quality. High quality data is the first step towards better predictions, which is why in practice data scientists spend much of their time on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ishmael 3608\n"
     ]
    }
   ],
   "source": [
    "for idx, (w1, w2) in enumerate(zip(training_words, training_words[1:])):\n",
    "    if w1 == \"Call\" and w2 == \"me\":\n",
    "        print \"Ishmael\", idx\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! That's pretty far down the page. Let's verify before moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'--WHARTON', u'THE', u'WHALE', u'KILLER.', u'\"So', u'be', u'cheery,', u'my', u'lads,', u'let', u'your', u'hearts', u'never', u'fail,', u'While', u'the', u'bold', u'harpooneer', u'is', u'striking', u'the', u'whale!\"', u'--NANTUCKET', u'SONG.', u'\"Oh,', u'the', u'rare', u'old', u'Whale,', u'mid', u'storm', u'and', u'gale', u'In', u'his', u'ocean', u'home', u'will', u'be', u'A', u'giant', u'in', u'might,', u'where', u'might', u'is', u'right,', u'And', u'King', u'of', u'the', u'boundless', u'sea.\"', u'--WHALE', u'SONG.', u'CHAPTER', u'1', u'Loomings.', u'Call', u'me', u'Ishmael.', u'Some', u'years', u'ago--never', u'mind', u'how', u'long', u'precisely--having', u'little', u'or', u'no', u'money', u'in', u'my', u'purse,', u'and', u'nothing', u'particular', u'to', u'interest', u'me', u'on', u'shore,', u'I', u'thought', u'I', u'would', u'sail', u'about', u'a', u'little', u'and', u'see', u'the', u'watery', u'part', u'of', u'the', u'world.', u'It']\n"
     ]
    }
   ],
   "source": [
    "print(training_words[3550:3650])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, looks good. Now let's zoom in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Call', u'me', u'Ishmael.', u'Some']\n"
     ]
    }
   ],
   "source": [
    "print(training_words[3608:3612])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, 3608 is definitely our starting point. And now we see another issue too. If we're going to generate plausible text, we need to take into account sentences not just words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chains\n",
    "Markov chains let us predict the probability one word will be followed by another word, mapping out a directed graph of state transitions between the different words in our body of text. In this way we find out something about grammar without having to encode it in our model. For more information, take a minute to read here: https://en.wikipedia.org/wiki/Markov_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(words):\n",
    "    last_word = \"_START\" # Initialize from a starting point\n",
    "    model = {\"_START\": {}}\n",
    "    for word in words:\n",
    "        try:\n",
    "            prev = model[last_word]\n",
    "        except KeyError:\n",
    "            model[last_word] = {word: 1}\n",
    "        else:\n",
    "            if word in prev:\n",
    "                prev[word] += 1\n",
    "            else:\n",
    "                prev[word] = 1\n",
    "    \n",
    "        if word.endswith('.') or word.endswith('!'):\n",
    "            last_word = \"_START\"\n",
    "        else:\n",
    "            last_word = word\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now that we've created a function to build our model, let's try it out and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28558\n"
     ]
    }
   ],
   "source": [
    "mk = build_model(training_words[3608:])\n",
    "print(len(mk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have a lot of top-level keys. This means there are 28558 states for which we've recorded transition frequencies. Let's check out our top starting words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'But', 582),\n",
       " (u'The', 395),\n",
       " (u'I', 278),\n",
       " (u'And', 265),\n",
       " (u'It', 231),\n",
       " (u'In', 190),\n",
       " (u'He', 168),\n",
       " (u'For', 154),\n",
       " (u'CHAPTER', 105),\n",
       " (u'A', 97)]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(mk[\"_START\"].items(), key=lambda (k,v): -v)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If they're all capitalized like this, we're doing well. Now let's see if we can do something a little more complicated. Let's find the top ranked pairs of words by using a two level list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pairs = sorted((-freq, (w1,w2)) for w1, follows in mk.items() for w2, freq in follows.items() if w1 != \"_START\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1821: of the\n",
      "1088: in the\n",
      "698: to the\n",
      "421: from the\n",
      "353: of his\n",
      "350: and the\n",
      "318: on the\n",
      "316: of a\n",
      "312: at the\n",
      "306: with the\n",
      "304: to be\n",
      "294: by the\n",
      "282: for the\n",
      "246: in his\n",
      "242: into the\n",
      "235: in a\n",
      "228: with a\n",
      "216: upon the\n",
      "211: that the\n",
      "196: as the\n",
      "193: all the\n",
      "166: out of\n",
      "165: it is\n",
      "162: it was\n",
      "157: for a\n",
      "155: the whale\n",
      "153: like a\n",
      "147: the same\n",
      "142: one of\n",
      "139: to his\n",
      "136: is the\n",
      "133: over the\n",
      "126: sort of\n",
      "123: was a\n",
      "122: as a\n",
      "122: as if\n",
      "121: of all\n",
      "121: such a\n",
      "116: in this\n",
      "116: of this\n",
      "113: of that\n",
      "110: had been\n",
      "110: have been\n",
      "109: is a\n",
      "109: to a\n",
      "108: and then\n",
      "107: with his\n",
      "106: the most\n",
      "104: I have\n",
      "104: the other\n"
     ]
    }
   ],
   "source": [
    "for freq, (w1, w2) in pairs[:50]:\n",
    "    print \"{}: {} {}\".format(-freq, w1, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know the top pairs, but we have to make **weighted random transitions**, not just list the top pairs. With a little help from numpy generating a cumulative sum of frequencies so we can tranform `[('The', 1), ('if', 7), ('as', 3)]` into `[('The', 1), ('if', 8), ('as', 11)]`, we can make a weighted random choice for the next word given a particular prior word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_choice(choices):\n",
    "    ch_list = list(sorted(choices.items(), key=lambda (k,v): v))  # [('w1', 1), ('w2', 3), ... ]\n",
    "    cs = list(numpy.cumsum([v for k, v in ch_list]))\n",
    "    cum_choices = [(cf,w) for cf, (w, f) in zip(cs, ch_list)]\n",
    "    choice = random.randint(1, cum_choices[-1][0])\n",
    "    for cf, value in cum_choices:\n",
    "        if choice <= cf:\n",
    "            return value\n",
    "    else:\n",
    "        raise ValueError(\"Random range out of bounds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = random_choice(mk[\"_START\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'The', 395)\n"
     ]
    }
   ],
   "source": [
    "print (x, mk['_START'][x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got a function to make random weighted choices, all that's left is to create our generation function. In the end, this is a simple algorithm:\n",
    "1. Have we generated enough words yet and is the current sentence over? If no, proceed to 2. If yes, finish.\n",
    "2. Is the previous word the end of a sentence or is there no previous word? Proceed to 3. Otherwise Proceed to 4.\n",
    "3. Generate random choice from sentence starting words. Proceed to 1.\n",
    "4. Generate random next word based on current state and add to end of word list. Proceed to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(min_words, weights):\n",
    "    words = []\n",
    "    while len(words) < min_words or not words[-1].endswith(\".\"):\n",
    "        \n",
    "        try:\n",
    "            prev = \"_START\" if words[-1].endswith(\".\") or words[-1].endswith(\"!\") else words[-1]\n",
    "        except IndexError:\n",
    "            prev = \"_START\"\n",
    "            \n",
    "        words.append(random_choice(weights[prev]))\n",
    "    return \" \".join(words)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But, by the creature as Ahab, as a wooden stock fish. But I seem to it, thou canst consume; but a word ROSE, and to his future ages, I was not been slily hovering over golden finger darted that mast, an inferior fellows all manner in my tambourine in the last come to him; but he did not.\n"
     ]
    }
   ],
   "source": [
    "print (generate(30, mk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Let's generate a longer one now. This might wrap in the middle of a word, so let's make use of Python's textwrap module to word wrap our output and generate a whole page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RECLINING AND ALL SORTS LYING ABOUT THE STERN WINDOWS; AHAB STANDING BEFORE HIS\n",
      "VICE-BENCH, AND BY HIS CAP.) It's worse than their leader originally educated\n",
      "according to some cases such fervent rays, that like a quaint craft steeply\n",
      "leaning against mine, when I saw him in it takes away then; pile themselves over\n",
      "it is ballasted with a perturbation. And now, I pass. Immediately the Jeroboam's\n",
      "boat where this thing he were lost their dreams of any path in the Bachelor's\n",
      "men sailors mark this ship--widows and that broke out to the carpenter here (by\n",
      "the Nantucketer, out of unknown to its far had voluntarily shipped for it to\n",
      "Nantucket captains of full of Birmah, forms and beheaded; and islets of it; far\n",
      "as little Johnny in the ear has de damndest row as he takes it be discovered.\n",
      "(RECLINING ON DECK; PIP CATCHES HIM BY THE WINDOW.) 'Twas rehearsed by and not\n",
      "rudely down, a panic; and tow dismasted frigates in vain remonstrating against\n",
      "the dark, purplish, yellow sea.* *That part of ships in keeping that all seasons\n",
      "a dismasted man at the old established family likeness! the cause, and\n",
      "wrinkles.\" \"It is an intelligent spirit that never mind how hard to another, and\n",
      "that you really beautiful and solitary twain; openly, and considering the van,\n",
      "this wide waters. For again for mere fright; and predestinated day was on the\n",
      "anvil--the red flag come down!\" When Stubb now calmly rose, and frankly thrust\n",
      "back upon thousands of the summer are wonders of the storm for the person I\n",
      "myself--that is the water-bearer, Flask; so much as the hump. What's that goney\n",
      "was gliding strangeness began to sacrifice of their bloated livers and thorough\n",
      "whaleman, you please, furnish to be facetious than billiard-tables, and spoke\n",
      "it; this whale.\" \"It was the White Whale, after breaking are not used in the way\n",
      "of their boats pursued, he must mount to be forced to the skeleton would then\n",
      "streamed behind him. Pray, what tune is glued in itself, and to understand, that\n",
      "Japanese seas have early oozed along it, as possible, watching the shore the\n",
      "crotch, and it is mystically carved in his wild ocean rolled. Blood and sent the\n",
      "oldest mariner who slights it. From the bottom of the whale, the heavens and\n",
      "thrown up the harpooneers of sight; an ill effect, of the whale, the harpooneer\n",
      "hears he is an air-freighted demijohn. I told him so. So that sight, completely\n",
      "digest even assuming the islands, battled with it; and portents and incidentally\n",
      "suggested to hand, as that club-hammer there on the revolving outer end to this\n",
      "earth. After many of superstitious probability. Do ye unless they are always\n",
      "among them, with great a question to him, wounding and whale-ports; this book,\n",
      "you say dat whale. You would find their unctuousness is whittled down in this;\n",
      "only clear Truth is his.\" \"We have for a sagacious savage, he will sometimes\n",
      "happens to quote:-- SACRED TO THE MAINMAST; STARBUCK APPROACHING HIM. It would\n",
      "come twenty feet long will derive the investigator can hold, where an interlude\n",
      "and all their own scythes--though in shades you burn, but on the circumstances,\n",
      "its rack, within his slippery back, and unexaggerating historian, except in his\n",
      "noon the benefit of gun-metal, stands there, a very much weary for the deck, and\n",
      "to be otherwise unaccountable odds the Fejee god made the desperate scene.\n"
     ]
    }
   ],
   "source": [
    "print( \"\\n\".join(textwrap.wrap(generate(500, mk), 80)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've done it! With no understanding of grammar or meaning, we've learned how to generate semi-plausible text. With some help from NLTK, we can improve this further by using stemming and punctuation detection to build a more complex probability distribution. NLTK can help a lot with this, but for now we'll stop here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
